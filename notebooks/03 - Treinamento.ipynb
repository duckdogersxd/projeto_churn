{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f83236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyggo\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "# Manipulação e Visualização de Dados\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay,accuracy_score, f1_score, recall_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01299eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/fold1_no_scale.csv')\n",
    "df_val = pd.read_csv('../data/fold2_no_scale.csv')\n",
    "df_test = pd.read_csv('../data/fold3_no_scale.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e6a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"Churn\"\n",
    "\n",
    "X_train, y_train = df_train.drop(columns=[TARGET]), df_train[TARGET]\n",
    "X_val,   y_val   = df_val.drop(columns=[TARGET]),   df_val[TARGET]\n",
    "X_test,  y_test  = df_test.drop(columns=[TARGET]),  df_test[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5822e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_column_names = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab67cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia Random Forest (validação): 0.7768762677484787\n",
      "Acurácia MLP (validação): 0.7870182555780934\n",
      "Acurácia XGBoost (validação): 0.7834685598377282\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_val)\n",
    "print(\"Acurácia Random Forest (validação):\", accuracy_score(y_val, rf_preds))\n",
    "\n",
    "# MLPClassifier\n",
    "mlp_model = MLPClassifier(random_state=42,max_iter=1500)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_preds = mlp_model.predict(X_val)\n",
    "print(\"Acurácia MLP (validação):\", accuracy_score(y_val, mlp_preds))\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_preds = xgb_model.predict(X_val)\n",
    "print(\"Acurácia XGBoost (validação):\", accuracy_score(y_val, xgb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8859b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metric(metric, y_true, probs, preds):\n",
    "#     \"\"\"\n",
    "#     Calcula o score de acordo com a métrica escolhida.\n",
    "    \n",
    "#     Parâmetros:\n",
    "#       - metric: string ('ks', 'roc', 'accuracy', 'f1', 'recall')\n",
    "#       - y_true: valores reais\n",
    "#       - probs: probabilidades preditas (usadas para KS e ROC-AUC)\n",
    "#       - preds: classes preditas (usadas para acurácia, F1 e recall)\n",
    "      \n",
    "#     Retorna:\n",
    "#       - Valor da métrica escolhida.\n",
    "#     \"\"\"\n",
    "#     if metric == 'ks':\n",
    "#         return ks_2samp(probs[y_true == 1], probs[y_true == 0]).statistic\n",
    "#     elif metric == 'roc':\n",
    "#         fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "#         return auc(fpr, tpr)\n",
    "#     elif metric == 'accuracy':\n",
    "#         return accuracy_score(y_true, preds)\n",
    "#     elif metric == 'f1':\n",
    "#         return f1_score(y_true, preds)\n",
    "#     elif metric == 'recall':\n",
    "#         return recall_score(y_true, preds)\n",
    "#     else:\n",
    "#         raise ValueError(\"Métrica não suportada. Escolha entre 'ks', 'roc', 'accuracy', 'f1' ou 'recall'.\")\n",
    "    \n",
    "# PARAM_SPACE = {\n",
    "#     \"rf\": {\n",
    "#         \"n_estimators\":           (50, 300,    \"int\"),\n",
    "#         \"max_depth\":              (2, 80,     \"int_log\"),\n",
    "#         \"min_samples_split\":      (2, 20,     \"int\"),\n",
    "#         \"min_samples_leaf\":       (1, 20,     \"int\"),\n",
    "#         \"max_features\":           (0.1, 1.0,  \"float\"),\n",
    "#         \"bootstrap\":              [True, False],\n",
    "#         \"criterion\":              [\"gini\", \"entropy\"],\n",
    "#     },\n",
    "#     \"mlp\": {\n",
    "#         \"hidden_layer_sizes\":     [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "#         \"activation\":             [\"relu\", \"tanh\", \"logistic\"],\n",
    "#         \"solver\":                 [\"adam\", \"sgd\"],\n",
    "#         \"alpha\":                  (1e-5, 1e-2,    \"float_log\"),\n",
    "#         \"learning_rate_init\":     (1e-7, 1e-1,    \"float_log\"),\n",
    "#         \"batch_size\":             [\"auto\", 8, 16, 32, 64, 128],\n",
    "#         \"max_iter\":               [ 500, 1000,1300,1500,2000],\n",
    "#         \"momentum\":               (0.0, 0.99,     \"float\"),\n",
    "#         \"learning_rate\":          [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "#         \"tol\":                    (1e-5, 1e-2,    \"float_log\"),\n",
    "#     },\n",
    "#     \"xgb\": {\n",
    "#         \"n_estimators\":           (50, 300,    \"int\"),\n",
    "#         \"max_depth\":              (2, 10,     \"int_log\"),\n",
    "#         \"learning_rate\":          (1e-3, 0.3,  \"float_log\"),\n",
    "#         \"gamma\":                  (0, 5,       \"float\"),\n",
    "#         \"subsample\":              (0.5, 1.0,   \"float\"),\n",
    "#         \"colsample_bytree\":       (0.5, 1.0,   \"float\"),\n",
    "#         \"reg_alpha\":              (1e-5, 1e-1, \"float_log\"),\n",
    "#         \"reg_lambda\":             (1e-5, 1e-1, \"float_log\"),\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "# def get_objective(model_name, metric=\"ks\"):\n",
    "#     space = PARAM_SPACE[model_name]\n",
    "#     def objective(trial):\n",
    "#         # sugere tudo dinamicamente\n",
    "#         kwargs = {}\n",
    "#         for k,v in space.items():\n",
    "#             if isinstance(v, tuple):\n",
    "#                 low, high, kind = v\n",
    "#                 if kind==\"int\":\n",
    "#                     kwargs[k] = trial.suggest_int(k, low, high)\n",
    "#                 elif kind==\"int_log\":\n",
    "#                     kwargs[k] = trial.suggest_int(k, low, high, log=True)\n",
    "#                 elif kind==\"float\":\n",
    "#                     kwargs[k] = trial.suggest_float(k, low, high)\n",
    "#                 elif kind==\"float_log\":\n",
    "#                     kwargs[k] = trial.suggest_float(k, low, high, log=True)\n",
    "#             else:\n",
    "#                 # lista de choices\n",
    "#                 kwargs[k] = trial.suggest_categorical(k, v)\n",
    "\n",
    "#         # instancia o modelo\n",
    "#         if model_name==\"rf\":\n",
    "#             clf = RandomForestClassifier(random_state=42, **kwargs)\n",
    "#             # fit & score\n",
    "#             clf.fit(X_train, y_train)\n",
    "#             probs = clf.predict_proba(X_val)[:,1]\n",
    "#             preds = clf.predict(X_val)\n",
    "#         elif model_name==\"mlp\":\n",
    "#             clf = MLPClassifier(random_state=42, **kwargs)\n",
    "#             scaler = StandardScaler()\n",
    "#             X_train_scaled = scaler.fit_transform(X_train)\n",
    "#             X_val_scaled = scaler.transform(X_val)\n",
    "#             # fit & score\n",
    "#             clf.fit(X_train_scaled, y_train)\n",
    "#             probs = clf.predict_proba(X_val_scaled)[:,1]\n",
    "#             preds = clf.predict(X_val_scaled)\n",
    "#         else:  # xgb\n",
    "#             clf = XGBClassifier(random_state=42, use_label_encoder=False,\n",
    "#                                 eval_metric=\"logloss\", **kwargs)\n",
    "#             # fit & score\n",
    "#             clf.fit(X_train, y_train)\n",
    "#             probs = clf.predict_proba(X_val)[:,1]\n",
    "#             preds = clf.predict(X_val)\n",
    "\n",
    "#         return compute_metric(metric, y_val, probs, preds)\n",
    "#     return objective\n",
    "\n",
    "# for model in [\"rf\",\"mlp\",\"xgb\"]:\n",
    "#     study = optuna.create_study(direction=\"maximize\")\n",
    "#     study.optimize(get_objective(model), n_trials=50)\n",
    "#     print(model, \"→ :best_params:\", study.best_params, \"| best_score:\", study.best_value)\n",
    "#     if model == \"rf\":\n",
    "#         best_rf_params = study.best_params\n",
    "#     elif model == \"mlp\":\n",
    "#         best_mlp_params = study.best_params\n",
    "#     elif model == \"xgb\":\n",
    "#         best_xgb_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17891e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_params = {'n_estimators': 181, 'max_depth': 80, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': 0.14433895866314056, 'bootstrap': True, 'criterion': 'gini'}\n",
    "best_mlp_params = {'hidden_layer_sizes': (50,), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00014922136338028723, 'learning_rate_init': 0.0013999067855568553, 'batch_size': 'auto', 'max_iter': 1000, 'momentum': 0.9079471829305951, 'learning_rate': 'constant', 'tol': 0.00279005126458061}\n",
    "best_xgb_params = {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.06394426113769631, 'gamma': 4.250873077353954, 'subsample': 0.8098870915836598, 'colsample_bytree': 0.8381743726836992, 'reg_alpha': 0.001466473948922872, 'reg_lambda': 0.002877321235259522}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4301ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_rf_params = {'n_estimators': 132, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 0.22290137215315986, 'bootstrap': True, 'criterion': 'entropy'}\n",
    "# best_mlp_params = {'hidden_layer_sizes': (100, 50), 'activation': 'tanh', 'solver': 'adam', 'alpha': 9.378890046495715e-05, 'learning_rate_init': 9.351095668258579e-06, 'batch_size': 16, 'max_iter': 1500, 'momentum': 0.7731362527089873, 'learning_rate': 'invscaling', 'tol': 4.0359613960011186e-05}\n",
    "# best_xgb_params = {'n_estimators': 181,\n",
    "#  'max_depth': 2,\n",
    "#  'learning_rate': 0.034086900335549314,\n",
    "#  'gamma': 1.6478134039856276,\n",
    "#  'subsample': 0.6339990719450903,\n",
    "#  'colsample_bytree': 0.6094061736031922,\n",
    "#  'reg_alpha': 0.004519046789752244,\n",
    "#  'reg_lambda': 0.0011747913628717445}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e4a63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyggo\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 'Random Forest' do Fold 1 salvo em: ../models/Random Forest_fold_1_model.joblib\n",
      "Modelo 'MLP' do Fold 1 salvo em: ../models/MLP_fold_1_model.joblib\n",
      "Modelo 'XGBoost' do Fold 1 salvo em: ../models/XGBoost_fold_1_model.joblib\n",
      "\n",
      "=== Fold 2 ===\n",
      "Modelo 'Random Forest' do Fold 2 salvo em: ../models/Random Forest_fold_2_model.joblib\n",
      "Modelo 'MLP' do Fold 2 salvo em: ../models/MLP_fold_2_model.joblib\n",
      "Modelo 'XGBoost' do Fold 2 salvo em: ../models/XGBoost_fold_2_model.joblib\n",
      "\n",
      "=== Fold 3 ===\n",
      "Modelo 'Random Forest' do Fold 3 salvo em: ../models/Random Forest_fold_3_model.joblib\n",
      "Modelo 'MLP' do Fold 3 salvo em: ../models/MLP_fold_3_model.joblib\n",
      "Modelo 'XGBoost' do Fold 3 salvo em: ../models/XGBoost_fold_3_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "SAVE_DIR = '../models/'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_valid_df = pd.DataFrame(X_val)\n",
    "X_test_df  = pd.DataFrame(X_test)\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "y_valid_df = pd.DataFrame(y_val)\n",
    "y_test_df  = pd.DataFrame(y_test)\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, **best_rf_params),\n",
    "    \"MLP\":           MLPClassifier(random_state=42, **best_mlp_params),\n",
    "    \"XGBoost\":       XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', **best_xgb_params)\n",
    "}\n",
    "\n",
    "folds = [\n",
    "    {\n",
    "        \"X_train\": pd.concat([X_train_df, X_valid_df], axis=0).reset_index(drop=True),\n",
    "        \"y_train\": pd.concat([y_train_df, y_valid_df], axis=0).reset_index(drop=True),\n",
    "        \"X_test\":  X_test_df.reset_index(drop=True),\n",
    "        \"y_test\":  y_test_df.reset_index(drop=True)\n",
    "    },\n",
    "    {\n",
    "        \"X_train\": pd.concat([X_valid_df, X_test_df], axis=0).reset_index(drop=True),\n",
    "        \"y_train\": pd.concat([y_valid_df, y_test_df], axis=0).reset_index(drop=True),\n",
    "        \"X_test\":  X_train_df.reset_index(drop=True),\n",
    "        \"y_test\":  y_train_df.reset_index(drop=True)\n",
    "    },\n",
    "    {\n",
    "        \"X_train\": pd.concat([X_test_df, X_train_df], axis=0).reset_index(drop=True),\n",
    "        \"y_train\": pd.concat([y_test_df, y_train_df], axis=0).reset_index(drop=True),\n",
    "        \"X_test\":  X_valid_df.reset_index(drop=True),\n",
    "        \"y_test\":  y_valid_df.reset_index(drop=True)\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, fold in enumerate(folds, start=1):\n",
    "    X_tr = fold[\"X_train\"]\n",
    "    y_tr = fold[\"y_train\"].squeeze().values.ravel()\n",
    "\n",
    "    print(f\"\\n=== Fold {i} ===\")\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_tr, y_tr)\n",
    "        filename = os.path.join(SAVE_DIR, f'{name}_fold_{i}_model.joblib')\n",
    "        joblib.dump(model, filename)\n",
    "        print(f\"Modelo '{name}' do Fold {i} salvo em: {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
